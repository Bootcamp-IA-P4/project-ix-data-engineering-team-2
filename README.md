# PROYECTO DATA ENGINEER: Construyendo un Pipeline ETL para HR Pro ğŸš€

## ğŸ“œ 1. Contexto del Proyecto: InmersiÃ³n en la IngenierÃ­a de Datos para HR Pro

Bienvenidos al emocionante desafÃ­o de DataTech Solutions. Nuestro cliente, HR Pro, una empresa lÃ­der en el sector de recursos humanos, se enfrenta al reto de gestionar y analizar volÃºmenes masivos de datos procedentes de diversas fuentes (solicitudes de empleo, registros de nÃ³mina, encuestas de empleados, entre otros). Como equipo de ingenieros de datos freelance, nuestra misiÃ³n ha sido diseÃ±ar e implementar un sistema de gestiÃ³n de datos eficiente que permita a HR Pro organizar y analizar esta valiosa informaciÃ³n.

Este proyecto se ha centrado en el desarrollo de un proceso ETL (ExtracciÃ³n, TransformaciÃ³n y Carga) robusto y escalable para integrar datos en un sistema unificado. Hemos trabajado con una amplia variedad de tipos de datos â€“desde informaciÃ³n personal y financiera hasta mÃ©tricas de rendimientoâ€“, asegurando su almacenamiento eficiente tanto en una base de datos NoSQL (MongoDB) como en un almacÃ©n de datos SQL (PostgreSQL en Supabase). Todo el sistema ha sido implementado en un entorno Dockerizado para garantizar la portabilidad y la facilidad de despliegue.

El resultado es un sistema funcional y exhaustivamente documentado, listo para potenciar la capacidad de HR Pro en la gestiÃ³n y anÃ¡lisis de sus datos de recursos humanos.

### âš ï¸ DISCLAIMER IMPORTANTE: RestricciÃ³n de Acceso al CÃ³digo Generador de Datos

Los datos utilizados en este proyecto NO SON REALES. Se generan de manera aleatoria y se envÃ­an a un servidor de Apache Kafka. El cÃ³digo fuente tanto para el servidor Kafka como para el generador de datos se encuentra en un repositorio de GitHub separado y estÃ¡ debidamente documentado.

Es ABSOLUTAMENTE FUNDAMENTAL que NO se acceda a leer el cÃ³digo que genera estos datos. Hacerlo supondrÃ­a conocer los detalles internos de su creaciÃ³n, lo cual se alejarÃ­a significativamente de un caso real y reducirÃ­a el aspecto pedagÃ³gico y desafiante de este proyecto. La intenciÃ³n detrÃ¡s de esta restricciÃ³n es fomentar el enfoque en los aspectos cruciales de la ingenierÃ­a de datos: la extracciÃ³n, transformaciÃ³n y carga, sin depender del conocimiento del origen de los datos.

Confiamos en que esta directriz serÃ¡ respetada, promoviendo una experiencia de aprendizaje honesta y valiosa.

## ğŸ—ï¸ 2. Arquitectura del Pipeline

El siguiente diagrama ilustra la arquitectura completa del pipeline de datos, desde la generaciÃ³n hasta la visualizaciÃ³n y consulta.

```mermaid
graph TD
    subgraph "Fuente de Datos (Externa)"
        A[Datagen Service]
    end

    subgraph "Plataforma de MensajerÃ­a"
        B[Apache Kafka]
    end

    subgraph "Procesamiento y Almacenamiento"
        C[Kafka Consumer]
        D[Redis]
        E[MongoDB]
        F[ETL: Mongo to Postgres]
        G[PostgreSQL on Supabase]
    end

    subgraph "MonitorizaciÃ³n y VisualizaciÃ³n"
        H[Prometheus]
        I[Grafana]
    end

    subgraph "Acceso a Datos"
        J[API RESTful]
        K[Frontend App]
    end

    A -- "EnvÃ­a datos JSON" --> B
    B -- "Topic: 'probando'" --> C
    C -- "Verifica duplicados" --> D
    C -- "Guarda datos Ãºnicos" --> E
    F -- "Extrae datos brutos" --> E
    F -- "Transforma y Unifica" --> F
    F -- "Carga datos estructurados" --> G
    J -- "Consulta datos" --> G
    K -- "Visualiza datos" --> J

    C -- "Expone MÃ©tricas" --> H
    F -- "Expone MÃ©tricas" --> H
    H -- "Fuente de Datos" --> I
```

## ğŸ¯ 3. Objetivos del Proyecto (Â¡Todos Cumplidos!)

Nuestro equipo ha alcanzado con Ã©xito todos los objetivos clave establecidos para este proyecto:

* âœ… **ImplementaciÃ³n de un proceso ETL completo:** Desde la ingesta en tiempo real hasta el almacenamiento relacional.
* âœ… **Preprocesamiento de datos exhaustivo:** Limpieza, unificaciÃ³n y manejo de inconsistencias.
* âœ… **Trabajo eficiente con colas de mensajes:** Consumo y gestiÃ³n de un stream de datos con Apache Kafka.
* âœ… **ImplementaciÃ³n y gestiÃ³n de bases de datos:** Uso de MongoDB (NoSQL) y PostgreSQL (SQL) para distintos propÃ³sitos dentro del pipeline.

## ğŸ“¦ 4. Condiciones de Entrega (Â¡Satisfactorias al 100%!)

Hemos cumplido con todas las condiciones de entrega del cliente, demostrando la solidez y completitud de nuestro trabajo:

* âœ… **Repositorio en GitHub** con el cÃ³digo fuente completamente documentado y siguiendo las mejores prÃ¡cticas de control de versiones.
* âœ… Un **programa dockerizado** que se conecta al servidor de Kafka, procesa los mensajes en tiempo real y los persiste de manera ordenada tanto en MongoDB como en PostgreSQL (gestionado a travÃ©s de Supabase).
* âœ… **Demo en vivo** mostrando el funcionamiento impecable de la aplicaciÃ³n, desde la generaciÃ³n de datos hasta su visualizaciÃ³n final.
* âœ… **PresentaciÃ³n tÃ©cnica completa**, explicando detalladamente los objetivos del proyecto, el proceso de desarrollo y las tecnologÃ­as implementadas.
* âœ… **Tablero Kanban** (utilizando plataformas como Trello o Jira) con una gestiÃ³n de proyecto clara y organizada.

## âš™ï¸ 5. TecnologÃ­as Utilizadas (Â¡El Stack de Nuestro Pipeline!)

Hemos utilizado un conjunto robusto de tecnologÃ­as para construir este pipeline de datos, aprovechando las fortalezas de cada una:

| TecnologÃ­a | AplicaciÃ³n en el Proyecto |
| :--- | :--- |
| `Git` / `GitHub` ğŸ™ | Control de versiones, gestiÃ³n de ramas organizada (main, dev, feature/*), commits atÃ³micos y descriptivos que documentan la evoluciÃ³n del proyecto. |
| `Docker` ğŸ³ / `Docker Compose` | OrquestaciÃ³n de todos los servicios clave del pipeline: servidores de Kafka y Zookeeper, instancias de MongoDB y PostgreSQL, asÃ­ como las herramientas de monitorizaciÃ³n Grafana y Prometheus. ContenedorizaciÃ³n individual para cada componente asegurando un entorno reproducible, aislado y portable. |
| `Python` ğŸ | Core del desarrollo: Scripts para la generaciÃ³n de datos, la lÃ³gica del consumidor de Kafka, los procesos de ExtracciÃ³n, TransformaciÃ³n y Carga (ETL), y la implementaciÃ³n de APIs. |
| `Apache Kafka` | Sistema de mensajerÃ­a distribuida para la ingesta de eventos de datos en tiempo real. |
| `Redis` | Base de datos en memoria utilizada para la **deduplicaciÃ³n de mensajes** del stream de Kafka, garantizando la idempotencia del proceso de ingesta. |
| `MongoDB` ğŸƒ (NoSQL) | Utilizada como un "Data Lake" para la ingesta inicial y el almacenamiento de datos brutos en su formato original, ofreciendo flexibilidad para datos semi-estructurados. |
| `PostgreSQL` ğŸ˜ (SQL vÃ­a Supabase) | Empleada como "Data Warehouse" para el almacenamiento de datos transformados, limpios, normalizados y agrupados, optimizados para anÃ¡lisis y consultas estructuradas. |
| `Prometheus` ğŸ”¥ | Utilizado para la recolecciÃ³n y agregaciÃ³n de mÃ©tricas de rendimiento de todos los componentes del pipeline. Configurado a travÃ©s de monitoring/prometheus/prometheus.yml. |
| `Grafana` ğŸ“ˆ | Ofrece paneles de control interactivos para visualizar en tiempo real mÃ©tricas clave como el consumo de mensajes, la velocidad de procesamiento, los tiempos de persistencia y el rendimiento general de la aplicaciÃ³n. |
| `API RESTful` (FastAPI) | Expone los datos consolidados de PostgreSQL para que puedan ser consultados por aplicaciones externas o un frontend. |
| `LibrerÃ­as Python` | `kafka-python`, `pymongo`, `supabase-py`, `redis`, `fastapi`, `faker`, `prometheus_client` |

## ğŸ† 6. Niveles de Entrega Alcanzados (Â¡Un Logro Integral!)

Este proyecto ha superado las expectativas al alcanzar todos los niveles de complejidad definidos, desde el esencial hasta el experto, lo que demuestra un dominio integral de las habilidades y prÃ¡cticas en IngenierÃ­a de Datos.

### ğŸŸ¢ Nivel Esencial:
* âœ… ConfiguraciÃ³n robusta del consumer de Kafka para procesar eficientemente miles de mensajes por segundo en tiempo real.
* âœ… Persistencia eficaz de los mensajes brutos de Kafka en una base de datos documental (MongoDB).
* âœ… Procesamiento y agrupaciÃ³n avanzada de datos de cada persona (datos personales, de ubicaciÃ³n, profesionales, bancarios y de red) en un Ãºnico registro consolidado, listo para el anÃ¡lisis.
* âœ… Persistencia estructurada y normalizada de los datos procesados y agrupados en una base de datos relacional (PostgreSQL en Supabase).
* âœ… Repositorio Git impecable con una organizaciÃ³n clara de ramas y commits atÃ³micos y descriptivos, adhiriÃ©ndose a las mejores prÃ¡cticas de versionado.
* âœ… DocumentaciÃ³n del cÃ³digo exhaustiva y un README detallado en GitHub.

### ğŸŸ¡ Nivel Medio:
* âœ… ImplementaciÃ³n de un sistema de logs sofisticado (`src/etl/utils/logg.py`, con logs organizados por fecha) para un seguimiento detallado, depuraciÃ³n y auditorÃ­a de los procesos del pipeline.
* âœ… InclusiÃ³n de tests unitarios (`tests/unit/test.py`) para garantizar la calidad, fiabilidad y robustez de los componentes crÃ­ticos del cÃ³digo.
* âœ… DockerizaciÃ³n completa de la aplicaciÃ³n utilizando Docker y Docker Compose (`docker-compose.yml`), asegurando un entorno de desarrollo y despliegue reproducible y altamente portable.

### ğŸŸ  Nivel Avanzado:
* âœ… MonitorizaciÃ³n integral y en tiempo real del funcionamiento del pipeline: seguimiento de mensajes consumidos, velocidad de procesamiento, tiempos de persistencia y rendimiento general de la aplicaciÃ³n. Esto se logrÃ³ mediante la exposiciÃ³n de mÃ©tricas personalizadas y el uso de Prometheus para la recolecciÃ³n y Grafana para la visualizaciÃ³n en dashboards interactivos.
* âœ… CreaciÃ³n de una API RESTful (`src/api`) robusta, diseÃ±ada para conectarse a la base de datos relacional (PostgreSQL) y permitir consultas eficientes sobre la informaciÃ³n final procesada, facilitando el acceso a los datos limpios y estructurados.

### ğŸ”´ Nivel Experto:
* âœ… AutomatizaciÃ³n completa de la carga de datos, asegurando que la informaciÃ³n en las bases de datos (MongoDB y PostgreSQL) se actualice de forma continua y en tiempo real, mientras el servidor de Kafka opera sin interrupciones y envÃ­a mensajes. Esto garantiza un flujo de datos dinÃ¡mico y auto-actualizable.
* âœ… Desarrollo de un frontend sencillo (por ejemplo, utilizando Streamlit o Gradio) que proporciona una interfaz intuitiva para consultar y visualizar los datos de los clientes disponibles en la base de datos SQL procesada, permitiendo una interacciÃ³n directa con los resultados del pipeline.

## ğŸ“Š 7. EvaluaciÃ³n y Relevancia del Proyecto

Este proyecto es una demostraciÃ³n prÃ¡ctica de la capacidad para abordar y resolver desafÃ­os complejos en IngenierÃ­a de Datos. Aborda directamente la capacidad de procesar y almacenar grandes volÃºmenes de datos de eventos en tiempo real, un criterio fundamental y altamente demandado en la industria moderna.

El manejo de esquemas de datos como Personal data, Location, Professional data, Bank Data y Net Data, que provienen de fuentes diversas y con la inherente variabilidad e inconsistencia de los datos del mundo real, ha requerido una cuidadosa unificaciÃ³n, limpieza y normalizaciÃ³n. La soluciÃ³n implementada es adaptable, versÃ¡til y robusta, validando la adquisiciÃ³n de habilidades prÃ¡cticas cruciales en el Ã¡mbito de la ingenierÃ­a de datos.

## ğŸš€ 8. GuÃ­a de InstalaciÃ³n y EjecuciÃ³n (Â¡Pon en Marcha el Pipeline Dockerizado!)

Para levantar todo el ecosistema de este pipeline de datos y ver el proceso en acciÃ³n en tu mÃ¡quina local, sigue estos sencillos pasos:

1. **Requisitos Previos:** AsegÃºrate de tener `Docker` y `Docker Compose` instalados en tu sistema. Si no los tienes, puedes seguir las instrucciones de instalaciÃ³n oficial en docs.docker.com.

2. **Variables de Entorno:** El servicio ETL necesita credenciales para conectarse a Supabase. Crea un archivo `.env` en la raÃ­z del proyecto y aÃ±ade tus credenciales:
    ```env
    # .env
    SUPABASE_URL="https://xxx.supabase.co"
    SUPABASE_SERVICE_ROLE_KEY="tu_super_clave_secreta"
    ```

3. **Clona este repositorio** en tu mÃ¡quina local:
    ```bash
    git clone [URL_DE_TU_REPOSitorio]
    cd nombre_de_tu_repositorio
    ```

4. **Inicia todos los servicios Docker** definidos en docker-compose.yml:
    ```bash
    docker-compose up --build
    ```
    * La primera vez que ejecutes este comando, el flag `--build` es necesario para construir las imÃ¡genes personalizadas de tus servicios.
    * Para futuras ejecuciones, si no has realizado cambios en el cÃ³digo de tus imÃ¡genes, puedes omitir `--build` para un inicio mÃ¡s rÃ¡pido: `docker-compose up`.
    * Si prefieres lanzar los servicios en segundo plano para liberar tu terminal, utiliza el flag `-d`: `docker-compose up -d`.

5. **Acceso a las Interfaces de Usuario de los Servicios:**
    * **Kafka UI** (Control de Temas y Mensajes): `http://localhost:8080` ğŸ“Š
    * **Mongo Express** (Explorador de MongoDB): `http://localhost:8081` ğŸƒ
    * **Prometheus** (MÃ©tricas del Pipeline): `http://localhost:9090` ğŸ”¥
    * **Grafana** (Dashboards de MonitorizaciÃ³n): `http://localhost:3000` ğŸ“ˆ (Usuario/ContraseÃ±a por defecto: `admin` / `admin` - se te pedirÃ¡ cambiarlos la primera vez).
    * **API Docs:** `http://localhost:8000/docs` (DocumentaciÃ³n interactiva de la API)

6. **Para detener los servicios:**
    ```bash
    docker-compose down
    ```

## ğŸ—‚ï¸ 9. Estructura de Datos Manejada: Entendiendo la Fuente

El proyecto gestiona datos JSON variados que simulan la complejidad y heterogeneidad de la informaciÃ³n de recursos humanos en un entorno real. Es fundamental destacar la necesidad de unificar y consolidar la informaciÃ³n de la misma persona que proviene de diferentes "fragmentos" de datos.

Los esquemas principales de los datos recibidos (pre-transformaciÃ³n) son:

* **Personal data**: `Name`, `Lastname`, `Sex`, `Telfnumber`, `Passport`, `E-Mail`
* **Location**: `Fullname`, `City`, `Address`
* **Professional data**: `Fullname`, `Company`, `Company Address`, `Company Telfnumber`, `Company E-Mail`, `Job`
* **Bank Data**: `Passport`, `IBAN`, `Salary`
* **Net Data**: `Address`, `IPv4`

**Nota Importante:** Parte del desafÃ­o del proceso ETL es identificar y unir los datos que pertenecen a la misma persona (utilizando el Passport como clave de uniÃ³n primaria) y manejar las posibles inconsistencias o variaciones en los datos crudos.

## ğŸ—„ï¸ 10. Estructura de las Bases de Datos

### Base de Datos NoSQL (MongoDB) ğŸƒ

La instancia de MongoDB actÃºa como un "Data Lake" o "Staging Area" para la ingesta inicial de los datos brutos y semi-estructurados que provienen del servidor de Kafka.

```
# Acceso a MongoDB desde el contenedor:
docker exec -it mongo mongosh

# Colecciones en MongoDB:
- personal_data
- location_data
- professional_data
- bank_data
- net_data
- unknown_type (para datos no clasificados)
```

Ejemplos de documentos en las colecciones:

```javascript
// personal_data
{
  _id: ObjectId('68514f7304425157718d667e'),
  passport: 'I97025217',
  email: 'margot36@yahoo.fr',
  last_name: 'Eduardo',
  name: 'JosÃ©',
  sex: [ 'ND' ],
  telfnumber: '001-696-430-9514x651'
}

// bank_data
{
  _id: ObjectId('68514f7304425157718d667b'),
  passport: 'I97025217',
  IBAN: 'GB58KBRG97307622390686',
  salary: '105651$'
}
```

### Base de Datos SQL (PostgreSQL en Supabase) ğŸ˜

AquÃ­ se almacena la informaciÃ³n ya procesada, limpia y consolidada. Este esquema relacional estÃ¡ optimizado para consultas analÃ­ticas y reportes.

```sql
-- DEFINICIÃ“N DE TABLAS EN SUPABASE (PostgreSQL)
CREATE TABLE public.locations (
    location_id SERIAL NOT NULL,
    address CHARACTER VARYING(255) NOT NULL,
    city CHARACTER VARYING(100) NOT NULL,
    CONSTRAINT locations_pkey PRIMARY KEY (location_id),
    CONSTRAINT uq_locations_address_city UNIQUE (address, city)
);

CREATE TABLE public.persons (
    passport CHARACTER VARYING(20) NOT NULL,
    first_name CHARACTER VARYING(100) NOT NULL,
    last_name CHARACTER VARYING(100) NOT NULL,
    sex CHARACTER VARYING(10) NULL,
    phone_number CHARACTER VARYING(20) NULL,
    email CHARACTER VARYING(100) NOT NULL,
    location_id_fk INTEGER NULL,
    CONSTRAINT persons_pkey PRIMARY KEY (passport),
    CONSTRAINT persons_email_key UNIQUE (email),
    CONSTRAINT fk_persons_location_id FOREIGN KEY (location_id_fk) REFERENCES locations (location_id)
);

CREATE TABLE public.bank_data (
    bank_id SERIAL NOT NULL,
    passport_fk CHARACTER VARYING(20) NOT NULL,
    iban CHARACTER VARYING(34) NOT NULL,
    salary CHARACTER VARYING(50) NULL,
    CONSTRAINT bank_data_pkey PRIMARY KEY (bank_id),
    CONSTRAINT bank_data_iban_key UNIQUE (iban),
    CONSTRAINT uq_bank_data_passport_fk UNIQUE (passport_fk),
    CONSTRAINT fk_bank_data_passport FOREIGN KEY (passport_fk) REFERENCES persons (passport)
);

CREATE TABLE public.network_data (
    network_id SERIAL NOT NULL,
    passport_fk CHARACTER VARYING(20) NOT NULL,
    location_id_fk INTEGER NULL,
    ip_address INET NOT NULL UNIQUE,
    CONSTRAINT network_data_pkey PRIMARY KEY (network_id),
    CONSTRAINT network_data_ip_address_key UNIQUE (ip_address),
    CONSTRAINT uq_network_data_passport_fk UNIQUE (passport_fk),
    CONSTRAINT fk_network_data_location_id FOREIGN KEY (location_id_fk) REFERENCES locations (location_id),
    CONSTRAINT fk_network_data_passport FOREIGN KEY (passport_fk) REFERENCES persons (passport)
);

CREATE TABLE public.professional_data (
    professional_id SERIAL NOT NULL,
    passport_fk CHARACTER VARYING(20) NOT NULL,
    company_name CHARACTER VARYING(100) NULL,
    company_address CHARACTER VARYING(255) NULL,
    company_phone_number CHARACTER VARYING(20) NULL,
    company_email CHARACTER VARYING(100) NULL,
    job_title CHARACTER VARYING(100) NULL,
    CONSTRAINT professional_data_pkey PRIMARY KEY (professional_id),
    CONSTRAINT uq_professional_data_passport_fk UNIQUE (passport_fk),
    CONSTRAINT fk_professional_data_passport FOREIGN KEY (passport_fk) REFERENCES persons (passport)
);
```

## ğŸŒ³ 11. Estructura del Repositorio

La organizaciÃ³n de este repositorio refleja la modularidad y las mejores prÃ¡cticas de un proyecto de ingenierÃ­a de datos, facilitando la colaboraciÃ³n y el mantenimiento.

```
.
â”œâ”€â”€ datagen/                            # ğŸ“¦ Componentes para la generaciÃ³n y envÃ­o de datos a Kafka
â”‚   â”œâ”€â”€ data_generator.py               # Script principal para generar datos aleatorios
â”‚   â”œâ”€â”€ Dockerfile                      # Dockerfile para contenedor del generador de datos
â”‚   â”œâ”€â”€ kafka_push.py                   # Script para enviar los datos generados al servidor Kafka
â”‚   â”œâ”€â”€ requirements.txt                # Dependencias Python especÃ­ficas del generador
â”‚   â””â”€â”€ wait-for-it.sh                  # Script de utilidad para esperar que los servicios estÃ©n listos
â”œâ”€â”€ docker-compose.yml                  # ğŸ³ Archivo principal para orquestar todos los servicios Docker
â”œâ”€â”€ docs/                               # ğŸ“ DocumentaciÃ³n adicional y scripts auxiliares
â”‚   â””â”€â”€ create_etl_structure_gitkeep.sh
â”œâ”€â”€ kafka_testing.py                    # Script para pruebas rÃ¡pidas de conexiÃ³n con Kafka
â”œâ”€â”€ LICENSE                             # ğŸ“œ Licencia del proyecto
â”œâ”€â”€ monitoring/                         # ğŸ“ˆ ConfiguraciÃ³n para la monitorizaciÃ³n con Prometheus y Grafana
â”‚   â”œâ”€â”€ grafana/
â”‚   â”‚   â”œâ”€â”€ grafana.db                  # Base de datos de Grafana
â”‚   â”‚   â”œâ”€â”€ plugins/                    # Directorio para plugins de Grafana
â”‚   â”‚   â”‚   â””â”€â”€ ... (archivos de plugins)
â”‚   â”‚   â”œâ”€â”€ provisioning/               # ConfiguraciÃ³n de DataSources y Dashboards
â”‚   â”‚   â”‚   â”œâ”€â”€ dashboards/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ dashboards.yml      # ConfiguraciÃ³n YAML para la carga de dashboards
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ full_pipeline_metrics.json  # Dashboard completo de mÃ©tricas del pipeline
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ pipeline_overview.json      # Dashboard de resumen y vista general del pipeline
â”‚   â”‚   â”‚   â””â”€â”€ datasources/
â”‚   â”‚   â”‚       â””â”€â”€ datasource.yml      # ConfiguraciÃ³n YAML para los DataSources de Grafana
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ prometheus/
â”‚       â””â”€â”€ prometheus.yml              # ConfiguraciÃ³n de Prometheus para recolectar mÃ©tricas de los servicios
â”œâ”€â”€ __pycache__/                        # Directorio para archivos bytecode de Python
â”œâ”€â”€ README.md                           # Este archivo (Â¡el que estÃ¡s leyendo!)
â”œâ”€â”€ requirements.txt                    # Dependencias Python globales del proyecto
â”œâ”€â”€ src/                                # âš™ï¸ CÃ³digo fuente de la aplicaciÃ³n principal
â”‚   â”œâ”€â”€ api/                            # Directorio para el cÃ³digo de la API RESTful (Nivel Experto)
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ etl/                            # Directorio principal del proceso ETL (TransformaciÃ³n y Carga)
â”‚   â”‚   â”œâ”€â”€ Dockerfile                  # Dockerfile para el servicio ETL
â”‚   â”‚   â”œâ”€â”€ Ejemplo_Doc.txt             # Archivo de ejemplo de documentaciÃ³n
â”‚   â”‚   â”œâ”€â”€ etl_utils.py                # Utilidades y funciones auxiliares para la transformaciÃ³n de datos
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ mongo_to_postgres.py        # LÃ³gica principal de migraciÃ³n de datos de MongoDB a PostgreSQL
â”‚   â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”‚   â”œâ”€â”€ records_tests/              # Archivos de ejemplo de datos combinados (CSV/JSON) para pruebas
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ remote_db_loader.py
â”‚   â”‚   â”œâ”€â”€ requirements.txt            # Dependencias Python especÃ­ficas del servicio ETL
â”‚   â”‚   â”œâ”€â”€ tools/                      # Herramientas auxiliares y scripts de anÃ¡lisis (ej. datos incompletos)
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ utils/                      # MÃ³dulos de utilidades internas (ej. logging)
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ logg.py                 # MÃ³dulo para el sistema de logs
â”‚   â”‚   â”‚   â””â”€â”€ logs/                   # Directorio para archivos de log generados
â”‚   â”‚   â”‚       â””â”€â”€ ...
â”‚   â”‚   â””â”€â”€ wait-for-it.sh              # Script de utilidad para esperar que los servicios estÃ©n listos
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ kafka_consumer/                 # ğŸ“¥ Directorio del consumidor de Kafka
â”‚   â”‚   â”œâ”€â”€ consumer.py                 # Script principal del consumidor de Kafka
â”‚   â”‚   â”œâ”€â”€ Dockerfile                  # Dockerfile para el contenedor del consumidor
â”‚   â”‚   â”œâ”€â”€ requirements.txt            # Dependencias Python especÃ­ficas del consumidor
â”‚   â”‚   â”œâ”€â”€ storage_mongo.py            # LÃ³gica para almacenar mensajes en MongoDB
â”‚   â”‚   â””â”€â”€ wait-for-it.sh              # Script de utilidad para esperar que los servicios estÃ©n listos
â”‚   â””â”€â”€ __pycache__/
â””â”€â”€ tests/                              # ğŸ§ª Pruebas del proyecto (unitarias e integraciÃ³n)
    â”œâ”€â”€ integration/                    # Pruebas de integraciÃ³n
    â””â”€â”€ unit/                           # Pruebas unitarias
        â”œâ”€â”€ __pycache__/
        â””â”€â”€ test.py                     # Archivo de tests unitarios
```
